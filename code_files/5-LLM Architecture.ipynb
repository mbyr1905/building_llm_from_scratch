{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPLEMENTING A GPT MODEL FROM SCRATCH TO GENERATE TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT ARCHITECTURE PART 1: DUMMY GPT MODEL CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        \n",
    "        # Use a placeholder for TransformerBlock\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        \n",
    "        # Use a placeholder for LayerNorm\n",
    "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        # A simple placeholder\n",
    "\n",
    "    def forward(self, x):\n",
    "        # This block does nothing and just returns its input.\n",
    "        return x\n",
    "\n",
    "\n",
    "class DummyLayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "        # The parameters here are just to mimic the LayerNorm interface.\n",
    "\n",
    "    def forward(self, x):\n",
    "        # This layer does nothing and just returns its input.\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 1: TOKENIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "batch = []\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 2: CREATE AN INSTANCE OF DUMMYGPTMODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[-1.2034,  0.3201, -0.7130,  ..., -1.5548, -0.2390, -0.4667],\n",
      "         [-0.1192,  0.4539, -0.4432,  ...,  0.2392,  1.3469,  1.2430],\n",
      "         [ 0.5307,  1.6720, -0.4695,  ...,  1.1966,  0.0111,  0.5835],\n",
      "         [ 0.0139,  1.6755, -0.3388,  ...,  1.1586, -0.0435, -1.0400]],\n",
      "\n",
      "        [[-1.0908,  0.1798, -0.9484,  ..., -1.6047,  0.2439, -0.4530],\n",
      "         [-0.7860,  0.5581, -0.0610,  ...,  0.4835, -0.0077,  1.6621],\n",
      "         [ 0.3567,  1.2698, -0.6398,  ..., -0.0162, -0.1296,  0.3717],\n",
      "         [-0.2407, -0.7349, -0.5102,  ...,  2.0057, -0.3694,  0.1814]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "logits = model(batch)\n",
    "print(\"Output shape:\", logits.shape)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "The output tensor has two rows corresponding to the two text samples. Each text sample\n",
    "consists of 4 tokens; each token is a 50,257-dimensional vector, which matches the size of\n",
    "the tokenizer's vocabulary.\n",
    "\n",
    "\n",
    "The embedding has 50,257 dimensions because each of these dimensions refers to a\n",
    "unique token in the vocabulary. At the end of this chapter, when we implement the\n",
    "postprocessing code, we will convert these 50,257-dimensional vectors back into token IDs,\n",
    "which we can then decode into words.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT ARCHITECTURE PART 2: LAYER NORMALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1115,  0.1204, -0.3696, -0.2404, -1.1969],\n",
      "        [ 0.2093, -0.9724, -0.7550,  0.3239, -0.1085]])\n",
      "tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
      "        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "batch_example = torch.randn(2, 5) #A\n",
    "print(batch_example)\n",
    "layer = nn.Sequential(nn.Linear(5, 6), nn.ReLU())\n",
    "out = layer(batch_example)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "The neural network layer we have coded consists of a Linear layer followed by a non-linear\n",
    "activation function, ReLU (short for Rectified Linear Unit), which is a standard activation\n",
    "function in neural networks. \n",
    "\n",
    "If you are unfamiliar with ReLU, it simply thresholds negative\n",
    "inputs to 0, ensuring that a layer outputs only positive values, which explains why the\n",
    "resulting layer output does not contain any negative values. \n",
    "\n",
    "(Note that we will use another,\n",
    "more sophisticated activation function in GPT, which we will introduce in the next section).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Before we apply layer normalization to these outputs, let's examine the mean and\n",
    "variance:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6])\n",
      "Mean:\n",
      " tensor([[0.1324],\n",
      "        [0.2170]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[0.0231],\n",
      "        [0.0398]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(out.shape)\n",
    "mean = out.mean(dim=-1, keepdim=True)\n",
    "var = out.var(dim=-1, keepdim=True)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized layer outputs:\n",
      " tensor([[ 0.6159,  1.4126, -0.8719,  0.5872, -0.8719, -0.8719],\n",
      "        [-0.0189,  0.1121, -1.0876,  1.5173,  0.5647, -1.0876]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "Mean:\n",
      " tensor([[9.9341e-09],\n",
      "        [0.0000e+00]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out_norm = (out - mean) / torch.sqrt(var)\n",
    "mean = out_norm.mean(dim=-1, keepdim=True)\n",
    "var = out_norm.var(dim=-1, keepdim=True)\n",
    "print(\"Normalized layer outputs:\\n\", out_norm)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[0.0000],\n",
      "        [0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.set_printoptions(sci_mode=False)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Let's now encapsulate this process in a PyTorch module that we can use in the GPT\n",
    "model later:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)#true-> bassesl correction\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1115,  0.1204, -0.3696, -0.2404, -1.1969],\n",
      "        [ 0.2093, -0.9724, -0.7550,  0.3239, -0.1085]])\n",
      "tensor([[ 0.5528,  1.0693, -0.0223,  0.2656, -1.8654],\n",
      "        [ 0.9087, -1.3767, -0.9564,  1.1304,  0.2940]], grad_fn=<AddBackward0>)\n",
      "Mean:\n",
      " tensor([[-0.0000],\n",
      "        [ 0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(batch_example)\n",
    "ln = LayerNorm(emb_dim=5)\n",
    "out_ln = ln(batch_example)\n",
    "print(out_ln)\n",
    "mean = out_ln.mean(dim=-1, keepdim=True)\n",
    "var = out_ln.var(dim=-1, unbiased=False, keepdim=True)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "As we can see based on the results, the layer normalization code works as expected and\n",
    "normalizes the values of each of the two inputs such that they have a mean of 0 and a\n",
    "variance of 1:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT ARCHITECTURE PART 3: FEEDFORWARD NEURAL NETWORK WITH GELU ACTIVATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) * \n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "To get an idea of what this GELU function looks like and how it compares to the ReLU\n",
    "function, let's plot these functions side by side:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "gelu, relu = GELU(), nn.ReLU()\n",
    "\n",
    "# Some sample data\n",
    "x = torch.linspace(-3, 3, 100)\n",
    "y_gelu, y_relu = gelu(x), relu(x)\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "for i, (y, label) in enumerate(zip([y_gelu, y_relu], [\"GELU\", \"ReLU\"]), 1):\n",
    "    plt.subplot(1, 2, i)\n",
    "    plt.plot(x, y)\n",
    "    plt.title(f\"{label} activation function\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(f\"{label}(x)\")\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "As we can see in the resulting plot, ReLU is a piecewise linear function that\n",
    "outputs the input directly if it is positive; otherwise, it outputs zero. \n",
    "\n",
    "GELU is a smooth, nonlinear function that approximates ReLU but with a non-zero gradient for negative values.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Next, let's use the GELU function to implement the small neural network module,\n",
    "FeedForward, that we will be using in the LLM's transformer block later:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),#expansion as expanded params are sent to GeLU \n",
    "            GELU(),# activation step\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),#Compression of previous expansaion\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n"
     ]
    }
   ],
   "source": [
    "print(GPT_CONFIG_124M[\"emb_dim\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Let's use the GELU function to implement the small neural network module,\n",
    "FeedForward, that we will be using in the LLM's transformer block later:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 768])\n"
     ]
    }
   ],
   "source": [
    "ffn = FeedForward(GPT_CONFIG_124M)\n",
    "x = torch.rand(2, 3, 768) #( batch_size, num_of_tokens, emb_dim)\n",
    "out = ffn(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT ARCHITECTURE PART 4: SHORTCUT CONNECTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleDeepNeuralNetwork(nn.Module):\n",
    "    def __init__(self, layer_sizes, use_shortcut):\n",
    "        super().__init__()\n",
    "        self.use_shortcut = use_shortcut\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]), GELU())\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            # Compute the output of the current layer\n",
    "            layer_output = layer(x)\n",
    "            # Check if shortcut can be applied\n",
    "            if self.use_shortcut and x.shape == layer_output.shape:\n",
    "                x = x + layer_output\n",
    "            else:\n",
    "                x = layer_output\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "The code implements a deep neural network with 5 layers, each consisting of a Linear\n",
    "layer and a GELU activation function. \n",
    "\n",
    "In the forward pass, we iteratively pass the input\n",
    "through the layers and optionally add the shortcut connections  if\n",
    "the self.use_shortcut attribute is set to True.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Let's use this code to first initialize a neural network without shortcut connections. Here,\n",
    "each layer will be initialized such that it accepts an example with 3 input values and returns\n",
    "3 output values. The last layer returns a single output value:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes = [3, 3, 3, 3, 3, 1]\n",
    "sample_input = torch.tensor([[1., 0., -1.]])\n",
    "torch.manual_seed(123) # specify random seed for the initial weights for reproducibility\n",
    "model_without_shortcut = ExampleDeepNeuralNetwork(\n",
    "layer_sizes, use_shortcut=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Next, we implement a function that computes the gradients in the the model's backward\n",
    "pass:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gradients(model, x):\n",
    "    # Forward pass\n",
    "    output = model(x)\n",
    "    target = torch.tensor([[0.]])\n",
    "\n",
    "    # Calculate loss based on how close the target\n",
    "    # and output are\n",
    "    loss = nn.MSELoss()\n",
    "    loss = loss(output, target)\n",
    "    \n",
    "    # Backward pass to calculate the gradients\n",
    "    loss.backward()\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            # Print the mean absolute gradient of the weights\n",
    "            print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.00020173587836325169\n",
      "layers.1.0.weight has gradient mean of 0.0001201116101583466\n",
      "layers.2.0.weight has gradient mean of 0.0007152041653171182\n",
      "layers.3.0.weight has gradient mean of 0.001398873864673078\n",
      "layers.4.0.weight has gradient mean of 0.005049646366387606\n"
     ]
    }
   ],
   "source": [
    "print_gradients(model_without_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Let's now instantiate a model with skip connections and see how it compares:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.2216978669166565\n",
      "layers.1.0.weight has gradient mean of 0.20694100856781006\n",
      "layers.2.0.weight has gradient mean of 0.3289698660373688\n",
      "layers.3.0.weight has gradient mean of 0.2665731906890869\n",
      "layers.4.0.weight has gradient mean of 1.3258538246154785\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model_with_shortcut = ExampleDeepNeuralNetwork(\n",
    "layer_sizes, use_shortcut=True\n",
    ")\n",
    "print_gradients(model_with_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe the output of previous block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT ARCHITECTURE PART 5: CODING ATTENTION AND LINEAR LAYERS IN A TRANSFORMER BLOCK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Let us code a transformer block as follows:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)#true-> bassesl correction\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) * \n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),#expansion as expanded params are sent to GeLU \n",
    "            GELU(),# activation step\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),#Compression of previous expansaion\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "            \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out#6 as same as d_in\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim(3)\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "                       diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        #b- batch size, num_tokens - number of tokens in that batch(3), d_in - input dim of each token(Here we took it to be 6)\n",
    "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) \n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        \n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2) \n",
    "        \n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec) # optional projection\n",
    "\n",
    "        return context_vec\n",
    "    \n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"], \n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Shortcut connection for attention block\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        # Shortcut connection for feed forward block\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 768])\n",
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "x = torch.rand(2, 4, 768) #A\n",
    "print(x.shape)\n",
    "block = TransformerBlock(GPT_CONFIG_124M)\n",
    "output = block(x)\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "The preservation of shape throughout the transformer block architecture is not incidental\n",
    "but a crucial aspect of its design. \n",
    "\n",
    "This design enables its effective application across a wide\n",
    "range of sequence-to-sequence tasks, where each output vector directly corresponds to an\n",
    "input vector, maintaining a one-to-one relationship. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT ARCHITECTURE PART 6: ENTIRE GPT MODEL ARCHITECTURE IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "The device setting will allow us to train the model on a CPU or GPU, depending on which device the input\n",
    "data sits\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        \n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        \n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch:\n",
      " tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "\n",
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[ 0.3613,  0.4222, -0.0711,  ...,  0.3483,  0.4661, -0.2838],\n",
      "         [-0.1792, -0.5660, -0.9485,  ...,  0.0477,  0.5181, -0.3168],\n",
      "         [ 0.7120,  0.0332,  0.1085,  ...,  0.1018, -0.4327, -0.2553],\n",
      "         [-1.0076,  0.3418, -0.1190,  ...,  0.7195,  0.4023,  0.0532]],\n",
      "\n",
      "        [[-0.2564,  0.0900,  0.0335,  ...,  0.2659,  0.4454, -0.6806],\n",
      "         [ 0.1230,  0.3653, -0.2074,  ...,  0.7705,  0.2710,  0.2246],\n",
      "         [ 1.0558,  1.0318, -0.2800,  ...,  0.6936,  0.3205, -0.3178],\n",
      "         [-0.1565,  0.3926,  0.3288,  ...,  1.2630, -0.1858,  0.0388]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "out = model(batch)\n",
    "print(\"Input batch:\\n\", batch)\n",
    "print(\"\\nOutput shape:\", out.shape)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "As we can see, the output tensor has the shape [2, 4, 50257], since we passed in 2 input\n",
    "texts with 4 tokens each. The last dimension, 50,257, corresponds to the vocabulary size of\n",
    "the tokenizer. In the next section, we will see how to convert each of these 50,257-\n",
    "dimensional output vectors back into tokens.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Using the numel() method, short for \"number of elements,\" we can collect the total\n",
    "number of parameters in the model's parameter tensors:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163009536\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(total_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "Earlier, we spoke of initializing a 124\n",
    "million parameter GPT model, so why is the actual number of parameters 163 million, as\n",
    "shown in the preceding code output?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "The reason is a concept called weight tying that is used in the original GPT-2\n",
    "architecture, which means that the original GPT-2 architecture is reusing the weights from\n",
    "the token embedding layer in its output layer. \n",
    "\n",
    "To understand what this means, let's take a\n",
    "look at the shapes of the token embedding layer and linear output layer that we initialized\n",
    "on the model via the GPTModel earlier:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token embedding layer shape: torch.Size([50257, 768])\n",
      "Output layer shape: torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "print(\"Token embedding layer shape:\", model.tok_emb.weight.shape)\n",
    "print(\"Output layer shape:\", model.out_head.weight.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "As we can see based on the print outputs, the weight tensors for both these layers have the\n",
    "same shape:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "The token embedding and output layers are very large due to the number of rows for the\n",
    "50,257 in the tokenizer's vocabulary. Let's remove the output layer parameter count from\n",
    "the total GPT-2 model count according to the weight tying:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters considering weight tying: 124,412,160\n"
     ]
    }
   ],
   "source": [
    "total_params_gpt2 = total_params - sum(p.numel() for p in model.out_head.parameters())\n",
    "print(f\"Number of trainable parameters considering weight tying: {total_params_gpt2:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of the model: 621.83 MB\n"
     ]
    }
   ],
   "source": [
    "total_size_bytes = total_params * 4 #A\n",
    "total_size_mb = total_size_bytes / (1024 * 1024) #B\n",
    "print(f\"Total size of the model: {total_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "In conclusion, by calculating the memory requirements for the 163 million parameters in\n",
    "our GPTModel object and assuming each parameter is a 32-bit float taking up 4 bytes, we\n",
    "find that the total size of the model amounts to 622 MB, illustrating the relatively large\n",
    "storage capacity required to accommodate even relatively small LLMs.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT ARCHITECTURE PART 7: GENERATING TEXT FROM OUTPUT TOKENS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "Step 1: idx is a (batch, n_tokens) array of indices in the current context\n",
    "\n",
    "Step 2: Crop current context if it exceeds the supported context size E.g., if LLM supports only 5 tokens, and the\n",
    "context size is 10 then only the last 5 tokens are used as context\n",
    "\n",
    "Step 3: Focus only on the last time step, so that (batch, n_token, vocab_size) becomes (batch, vocab_size)\n",
    "\n",
    "Step 4: probas has shape (batch, vocab_size)\n",
    "\n",
    "Step 5: idx_next has shape (batch, 1)\n",
    "\n",
    "Step 6: Append sampled index to the running sequence, where idx has shape (batch, n_tokens+1)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # idx is (batch, n_tokens) array of indices in the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "        \n",
    "        # Crop current context if it exceeds the supported context size\n",
    "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
    "        # then only the last 5 tokens are used as context\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        \n",
    "        # Get the predictions\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        \n",
    "        # Focus only on the last time step\n",
    "        # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n",
    "        logits = logits[:, -1, :]  \n",
    "\n",
    "        # Apply softmax to get probabilities\n",
    "        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n",
    "\n",
    "        # Get the idx of the vocab entry with the highest probability value\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n",
    "\n",
    "        # Append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "In the next chapter, when we will implement the GPT training code, we will also\n",
    "introduce additional sampling techniques where we modify the softmax outputs such that\n",
    "the model doesn't always select the most likely token, which introduces variability and\n",
    "creativity in the generated text.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded: [15496, 11, 314, 716]\n",
      "encoded_tensor.shape: torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "start_context = \"Hello, I am\"\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(\"encoded:\", encoded)\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0) #A\n",
    "print(\"encoded_tensor.shape:\", encoded_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Next, we put the model into .eval() mode, which disables random components like\n",
    "dropout, which are only used during training, and use the generate_text_simple function\n",
    "on the encoded input tensor:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "We disable dropout since we are not training the model\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor([[15496,    11,   314,   716, 27018, 24086, 47843, 30961, 42348,  7267]])\n",
      "Output length: 10\n",
      "Hello, I am Featureiman Byeswickattribute argue\n"
     ]
    }
   ],
   "source": [
    "model.eval() #A\n",
    "out = generate_text_simple(\n",
    "model=model,\n",
    "idx=encoded_tensor,\n",
    "max_new_tokens=6,\n",
    "context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "print(\"Output:\", out)\n",
    "print(\"Output length:\", len(out[0]))\n",
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(decoded_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_scratch_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
