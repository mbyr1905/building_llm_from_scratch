{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING LOOP FOR THE LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 256, # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, \n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn   \n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)#true-> bassesl correction\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) * \n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),#expansion as expanded params are sent to GeLU \n",
    "            GELU(),# activation step\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),#Compression of previous expansaion\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "            \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out#6 as same as d_in\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim(3)\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "                       diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        #b- batch size, num_tokens - number of tokens in that batch(3), d_in - input dim of each token(Here we took it to be 6)\n",
    "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) \n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        \n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2) \n",
    "        \n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec) # optional projection\n",
    "\n",
    "        return context_vec\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"], \n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Shortcut connection for attention block\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        # Shortcut connection for feed forward block\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        return x\n",
    "    \n",
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # idx is (batch, n_tokens) array of indices in the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "        \n",
    "        # Crop current context if it exceeds the supported context size\n",
    "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
    "        # then only the last 5 tokens are used as context\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        \n",
    "        # Get the predictions\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        \n",
    "        # Focus only on the last time step\n",
    "        # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n",
    "        logits = logits[:, -1, :]  \n",
    "\n",
    "        # Apply softmax to get probabilities\n",
    "        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n",
    "\n",
    "        # Get the idx of the vocab entry with the highest probability value\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n",
    "\n",
    "        # Append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        \n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        \n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval();  # Disable dropout during inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import urllib.request\n",
    "\n",
    "file_path = \"../pdfs/the-verdict.txt\"\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    text_data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "# Train/validation ratio\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "# print(val_data)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    # Initialize lists to track losses and tokens seen\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        \n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward() # Calculate loss gradients\n",
    "            optimizer.step() # Update model weights using loss gradients\n",
    "            tokens_seen += input_batch.numel() # Returns the total number of elements (or tokens) in the input_batch.\n",
    "            global_step += 1\n",
    "\n",
    "            # Optional evaluation step\n",
    "            if global_step % eval_freq == 0: \n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        # Print a sample text after each epoch\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "Step 1: Initialize lists to track losses and tokens seen\n",
    "\n",
    "Step 2: Start the main training loop\n",
    "\n",
    "Step 3: Reset loss gradients from previous batch iteration\n",
    "\n",
    "Step 4: Calculate loss gradients\n",
    "\n",
    "Step 5: Update model weights using loss gradients\n",
    "\n",
    "Step 6: Optional evaluation step\n",
    "\n",
    "Step 7: Print a sample text after each epoch\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 9.781, Val loss 9.933\n",
      "Ep 1 (Step 000005): Train loss 8.111, Val loss 8.339\n",
      "Every effort moves you,,,,,,,,,,,,.                                     \n",
      "Ep 2 (Step 000010): Train loss 6.661, Val loss 7.048\n",
      "Ep 2 (Step 000015): Train loss 5.961, Val loss 6.616\n",
      "Every effort moves you, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and,, and, and,\n",
      "Ep 3 (Step 000020): Train loss 5.726, Val loss 6.600\n",
      "Ep 3 (Step 000025): Train loss 5.201, Val loss 6.348\n",
      "Every effort moves you, and I had been.                                            \n",
      "Ep 4 (Step 000030): Train loss 4.417, Val loss 6.278\n",
      "Ep 4 (Step 000035): Train loss 4.069, Val loss 6.226\n",
      "Every effort moves you know the                          \"I he had the donkey and I had the and I had the donkey and down the room, I had\n",
      "Ep 5 (Step 000040): Train loss 3.732, Val loss 6.160\n",
      "Every effort moves you know it was not that the picture--I had the fact by the last I had been--his, and in the            \"Oh, and he said, and down the room, and in\n",
      "Ep 6 (Step 000045): Train loss 2.850, Val loss 6.179\n",
      "Ep 6 (Step 000050): Train loss 2.427, Val loss 6.141\n",
      "Every effort moves you know,\" was one of the picture. The--I had a little of a little: \"Yes, and in fact, and in the picture was, and I had been at my elbow and as his pictures, and down the room, I had\n",
      "Ep 7 (Step 000055): Train loss 2.104, Val loss 6.134\n",
      "Ep 7 (Step 000060): Train loss 1.882, Val loss 6.233\n",
      "Every effort moves you know,\" was one of the picture for nothing--I told Mrs.  \"I was no--as! The women had been, in the moment--as Jack himself, as once one had been the donkey, and were, and in his\n",
      "Ep 8 (Step 000065): Train loss 1.320, Val loss 6.238\n",
      "Ep 8 (Step 000070): Train loss 0.985, Val loss 6.242\n",
      "Every effort moves you know,\" was one of the axioms he had been the tips of a self-confident moustache, I felt to see a smile behind his close grayish beard--as if he had the donkey. \"strongest,\" as his\n",
      "Ep 9 (Step 000075): Train loss 0.717, Val loss 6.293\n",
      "Ep 9 (Step 000080): Train loss 0.541, Val loss 6.393\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back the window-curtains, I had the donkey. \"There were days when I\n",
      "Ep 10 (Step 000085): Train loss 0.391, Val loss 6.452\n",
      "Every effort moves you know,\" was one of the axioms he laid down across the Sevres and silver of an exquisitely appointed luncheon-table, when, on a later day, I had again run over from Monte Carlo; and Mrs. Gis\n",
      "Training completed in 16.04 minutes.\n"
     ]
    }
   ],
   "source": [
    "# Note:\n",
    "# Uncomment the following code to calculate the execution time\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "device=torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Note:\n",
    "# Uncomment the following code to show the execution time\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    # Plot training and validation loss against epochs\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
    "\n",
    "    # Create a second x-axis for tokens seen\n",
    "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "    fig.tight_layout()  # Adjust layout to make room\n",
    "    plt.savefig(\"loss-plot.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Both the training and validation losses start to improve for the first\n",
    "epoch. However, the losses start to diverge past the second epoch. \n",
    "\n",
    "This divergence and the\n",
    "fact that the validation loss is much larger than the training loss indicate that the model is\n",
    "overfitting to the training data. \n",
    "\n",
    "We can confirm that the model memorizes the training data\n",
    "verbatim by searching for the generated text snippets, such as \"quite insensible to the\n",
    "irony\" in the \"The Verdict\" text file.\n",
    "\n",
    "\n",
    "This memorization is expected since we are working with a very, very small training\n",
    "dataset and training the model for multiple epochs. \n",
    "\n",
    "Usually, it's common to train a model\n",
    "on a much, much larger dataset for only one epoch.   \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DECODING STRATEGIES TO CONTROL RANDOMNESS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "First, we briefly revisit the generate_text_simple function\n",
    "from the previous chapter that we used inside the generate_and_print_sample earlier in\n",
    "this chapter. \n",
    "\n",
    "Then, we will cover two techniques, temperature scaling, and top-k sampling,\n",
    "to improve this function.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cpu\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you know,\" was one of the axioms he laid down across the Sevres and silver of an exquisitely appointed lun\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DECODING STRATEGY 1: TEMPERATURE SCALING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = { \n",
    "    \"closer\": 0,\n",
    "    \"every\": 1, \n",
    "    \"effort\": 2, \n",
    "    \"forward\": 3,\n",
    "    \"inches\": 4,\n",
    "    \"moves\": 5, \n",
    "    \"pizza\": 6,\n",
    "    \"toward\": 7,\n",
    "    \"you\": 8,\n",
    "} \n",
    "\n",
    "inverse_vocab = {v: k for k, v in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token_logits = torch.tensor(\n",
    "[4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n"
     ]
    }
   ],
   "source": [
    "probas = torch.softmax(next_token_logits, dim=0)\n",
    "next_token_id = torch.argmax(probas).item()\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "next_token_id = torch.multinomial(probas, num_samples=1).item()\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73 x closer\n",
      "0 x every\n",
      "0 x effort\n",
      "582 x forward\n",
      "2 x inches\n",
      "0 x moves\n",
      "0 x pizza\n",
      "343 x toward\n"
     ]
    }
   ],
   "source": [
    "def print_sampled_tokens(probas):\n",
    "    torch.manual_seed(123) # Manual seed for reproducibility\n",
    "    sample = [torch.multinomial(probas, num_samples=1).item() for i in range(1_000)]\n",
    "    sampled_ids = torch.bincount(torch.tensor(sample))\n",
    "    for i, freq in enumerate(sampled_ids):\n",
    "        print(f\"{freq} x {inverse_vocab[i]}\")\n",
    "\n",
    "print_sampled_tokens(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_with_temperature(logits, temperature):\n",
    "    scaled_logits = logits / temperature\n",
    "    return torch.softmax(scaled_logits, dim=0)\n",
    "\n",
    "# Temperature values\n",
    "temperatures = [1, 0.1, 5]  # Original, higher confidence, and lower confidence\n",
    "\n",
    "# Calculate scaled probabilities\n",
    "scaled_probas = [softmax_with_temperature(next_token_logits, T) for T in temperatures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting\n",
    "x = torch.arange(len(vocab))\n",
    "bar_width = 0.15\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "for i, T in enumerate(temperatures):\n",
    "    rects = ax.bar(x + i * bar_width, scaled_probas[i], bar_width, label=f'Temperature = {T}')\n",
    "\n",
    "ax.set_ylabel('Probability')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(vocab.keys(), rotation=90)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"temperature-plot.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DECODING STRATEGY 2: Top-k sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top logits: tensor([6.7500, 6.2800, 4.5100])\n",
      "Top positions: tensor([3, 7, 0])\n"
     ]
    }
   ],
   "source": [
    "top_k = 3\n",
    "top_logits, top_pos = torch.topk(next_token_logits, top_k)\n",
    "print(\"Top logits:\", top_logits)\n",
    "print(\"Top positions:\", top_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.5100,   -inf,   -inf, 6.7500,   -inf,   -inf,   -inf, 6.2800,   -inf])\n"
     ]
    }
   ],
   "source": [
    "new_logits = torch.where(\n",
    "    condition=next_token_logits < top_logits[-1],\n",
    "    input=torch.tensor(float(\"-inf\")), \n",
    "    other=next_token_logits\n",
    ")\n",
    "\n",
    "print(new_logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Lastly, let's apply the softmax function to turn these into next-token probabilities:\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0615, 0.0000, 0.0000, 0.5775, 0.0000, 0.0000, 0.0000, 0.3610, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "topk_probas = torch.softmax(new_logits, dim=0)\n",
    "print(topk_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Temperature Scaling and Top-k sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "We can now apply the temperature scaling and multinomial function for probabilistic\n",
    "sampling introduced in the previous section to select the next token among these 3 nonzero probability scores to generate the next token. We do this in the next section by\n",
    "modifying the text generation function.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "The previous two subsections introduced two concepts to increase the diversity of LLMgenerated text: temperature sampling and top-k sampling. In this section, we combine and\n",
    "add these concepts to modify the generate_simple function we used to generate text via\n",
    "the LLM earlier, creating a new generate function:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "Step 1: For-loop is the same as before: Get logits, and only focus on last time step\n",
    "\n",
    "Step 2: In this new section, we filter logits with top_k sampling\n",
    "\n",
    "Step 3: This is the new section where we apply temperature scaling\n",
    "    \n",
    "Step 4: Carry out greedy next-token selection as before when temperature scaling is disabled\n",
    "\n",
    "Step 5: Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "\n",
    "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # New: Filter logits with top_k sampling\n",
    "        if top_k is not None:\n",
    "            # Keep only top_k values\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
    "\n",
    "        # New: Apply temperature scaling\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
    "\n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
    "\n",
    "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
    "\n",
    "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
    "            break\n",
    "\n",
    "        # Same as before: append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Let's now see this new generate function in action:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you stand to work on surprise, a one of us had gone with random-\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=25,\n",
    "    temperature=1.4\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "As we can see, the generated text is very different from the one we previously generated\n",
    "via the generate_simple function at the beginning of section 5.3 (\"Every effort moves\n",
    "you know,\" was one of the axioms he laid...!\"), which was a memorized passage\n",
    "from the training set.\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_scratch_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
